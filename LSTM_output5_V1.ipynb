{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 18 16:06:22 2018\n",
    "\n",
    "@author: Ken\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "def normalize(df):\n",
    "    newdf= df.copy()\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    for feature in df.columns:\n",
    "        newdf[feature] = min_max_scaler.fit_transform(df[feature].values.reshape(-1,1))\n",
    "    return newdf\n",
    "    \n",
    "def data_helper(df, time_frame, train_rate):\n",
    "    # Feature number\n",
    "    number_features = len(df.columns)\n",
    "    datavalue = df.as_matrix()\n",
    "    x_result = []    \n",
    "    y_result = []\n",
    "    for index in range(len(datavalue)-time_frame):\n",
    "        x_result.append(datavalue[index: index + time_frame])\n",
    "        y_result.append(datavalue[index + time_frame][:5])  # test for all value\n",
    "    x_result = np.array(x_result)\n",
    "    y_result = np.array(y_result)\n",
    "\n",
    "    number_train = round((1 - train_rate) * x_result.shape[0])\n",
    "    x_train, x_test = x_result[:int(number_train)], x_result[int(number_train) :]\n",
    "    y_train, y_test = y_result[:int(number_train)], y_result[int(number_train) :]\n",
    "    \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], number_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], number_features))\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "def build_model(input_length, df, cell, layers, dropout):\n",
    "    print(len(df.columns))\n",
    "    number_features = len(df.columns)\n",
    "    model = Sequential()\n",
    "    for _ in range(layers):\n",
    "        model.add(LSTM(cell, input_shape=(input_length, number_features), return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(LSTM(cell, input_shape=(input_length, number_features), return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(16,kernel_initializer=\"uniform\",activation='relu'))\n",
    "    model.add(Dense(5,kernel_initializer=\"uniform\",activation='linear')) # CELL_SIZE 5, output dim\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy']) # mae\n",
    "    return model\n",
    "\n",
    "def denormalize(df, norm_value):\n",
    "    denorm_value = []\n",
    "\n",
    "    #norm_value = norm_value.T\n",
    "\n",
    "    pre_df = pd.DataFrame(norm_value, columns = df.columns[:5])\n",
    "    for feature in df.columns[:5]:\n",
    "        \n",
    "        original_value = df[feature].values.reshape(-1,1)\n",
    "        denorm_column = pre_df[feature].values.reshape(-1,1)\n",
    "    \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        min_max_scaler.fit_transform(original_value)\n",
    "        denorm_value.append(min_max_scaler.inverse_transform(denorm_column))\n",
    "        \n",
    "    return denorm_value\n",
    "\n",
    "def generate_model(parameters, df, csvfile):\n",
    "    time_step, train_rate, cell_size, layers_num, batchsize, epochs_num, dropout_rate, activation_1, activation_2, validation_rate = parameters\n",
    "    df_norm= normalize(df)\n",
    "    X_train, y_train, X_test, y_test = data_helper(df_norm, time_step, train_rate)\n",
    "    model = build_model(time_step, df, cell_size, layers_num, dropout_rate)\n",
    "    model.fit( X_train, y_train, batch_size=batchsize, epochs=epochs_num, validation_split=validation_rate, verbose=1)\n",
    "    \n",
    "    pred = model.predict(X_test) # model.predict(data set)\n",
    "\n",
    "    denorm_pred = denormalize(df, pred)\n",
    "    denorm_ytest = denormalize(df, y_test)\n",
    "    \n",
    "    #print(model.summary()) # model summary\n",
    "    #print(model.get_config()) # model configuration\n",
    "    model_path = csvfile[:-4] + '_model.md' \n",
    "    model.save(model_path)\n",
    "    \n",
    "    #%matplotlib inline  \n",
    "    for i in range(5):\n",
    "        plt.plot(denorm_pred[i],color='red', label='Prediction')\n",
    "        plt.plot(denorm_ytest[i],color='blue', label='Answer')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    score = model.evaluate(X_test, y_test, batch_size=batchsize)\n",
    "    print(denorm_pred)\n",
    "    return score[0]\n",
    "\n",
    "def main():\n",
    "    #chromosom_value=[[10,20,30],[0.1,0.2,0.3],[16,32,64,128,256],[1,2,3],[100,200,300],[100,200,300],[0.1,0.2,0.3,0.4,0.5],[\"sigmoid\",\"relu\",\"linear\"],[\"sigmoid\",\"relu\",\"linear\"],[0.1,0.2,0.3]]\n",
    "    # 1. TIME_FRAME = [10,20,30] # input資料維度\n",
    "    # 2. TRAIN_RATE = [0.1,0.2,0.3] # 切割train, test資料比例\n",
    "    # 3. CELL_SIZE = ：[16,32,64,128,256] # hidden Layer中memory cells\n",
    "    # 4. LAYERS_NUM = ：[1,2,3] # Layer數\n",
    "    # 5. BATCH_SIZE = ：[100,200,300] # 每次批次投入資料量\n",
    "    # 6. EPOCHS_NUM =  [100,200,300] # 迭代數\n",
    "    # 7. DROPOUT_RATE = [0.1,0.2,0.3,0.4,0.5]3 # Dropout 比例\n",
    "    # 8. activation = '' # 第一層 activation ：[\"sigmoid\",\"relu\",\"linear\"],\n",
    "    # 9. activation = '' # 第二層 activation ：[\"sigmoid\",\"relu\",\"linear\"],\n",
    "    # 10.validation rate ：[0.1,0.2,0.3]\n",
    "    ''' load all csv\n",
    "    csv_files =[i for i in fnmatch.filter(os.listdir('.'), '*.csv')]\n",
    "    for file in csv_files:\n",
    "        foxconndf= pd.read_csv(file, index_col=0 )\n",
    "        foxconndf.dropna(how='any',inplace=True) # remove the rows with blank\n",
    "        foxconndf = foxconndf.drop(['Date'], axis=1)\n",
    "        # 調整 foxconndf columns\n",
    "    ''' \n",
    "\n",
    "    foxconndf= pd.read_csv('0051.csv', index_col=0 )\n",
    "    foxconndf.dropna(how='any',inplace=True) # remove the rows with blank\n",
    "    foxconndf = foxconndf.drop(['Date'], axis=1)\n",
    "    # 調整 foxconndf columns\n",
    "    test = [5,0.1,50,2,None,100,0.3,'relu','linear', 0.1] # 參數list範例\n",
    "    score = generate_model(test, foxconndf, '0051.csv')\n",
    "    print(score)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
